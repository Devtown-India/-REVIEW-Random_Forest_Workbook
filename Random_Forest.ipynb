{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests - Introduction:\n",
    "\n",
    "The Random Forest algorithm is a supervised machine learning algorithm that can be used for both classification and regression problems.The basic unit of a Random Forest model is a Decision Tree. As the name suggests, this algorithm randomly creates and merges multiple decision trees into one \"forest\". It uses *bagging* and *feature randomness* while building individual trees to try and create an **uncorrelated** forest whose prediction is more accurate than that of any individual tree.\n",
    "\n",
    "### Advantages of Random Forests:\n",
    "\n",
    "1. No overfitting of data.\n",
    "\n",
    "2. Can be used for both regression and classification problems unlike most other algorithms.\n",
    "\n",
    "3. Feature scaling not required.\n",
    "\n",
    "4. Robust to outliers.\n",
    "\n",
    "5. Comparatively less affected by noise.\n",
    "\n",
    "6. Can handle missing values.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests - Working:\n",
    "\n",
    "The fundamental idea behind a random forest, as mentioned above, is to combine many decision trees into a single model. Individually, predictions made by decision trees may not be accurate, but combined together, the prediction will be closer to the mark on average. This is because, predictions of individual trees ( if they are not overfit ) have variance, i.e, they are widely spread around the right answer. But, when multiple trees are combined together, the overall prediction almost always has lesser variance and higher accuracy.\n",
    "\n",
    "### Bagging ( Bootstrap Aggregation):\n",
    "Bootstrap Aggregation is a simple and very powerful ensemble method (a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model) that can be used to reduce the variance for those algorithm that have high variance.\n",
    "\n",
    "Eg: Consider a sample dataset of 1000 instances.Bagging works as follows:\n",
    "\n",
    "1. Create 100 sub-samples of the dataset.\n",
    "2. Train a decision tree model on each of those sub-samples.\n",
    "3. Calculate the average prediction.\n",
    "\n",
    "For example, If 5 different trees made the class predictions blue, blue, green, blue and red, the final prediction becomes blue.\n",
    "\n",
    "![alt text](./images/random_forest.JPG \"Title\")\n",
    "\n",
    "### Feature Randomness:\n",
    "The problem with using Bagging on Decision trees is that Decision trees are greedy. During splitting of a node, they choose which variable to split on using a greedy algorithm that minimizes error, i.e, they consider every possible feature and pick the one that produces the most separation between the observations in the left node vs. those in the right node. This means that, almost all the individual trees are bound to choose the same set of split points thus leading to similar predictions (or) The trees have a high correlation.<br /> \n",
    "To avoid this, the random forest algorithm changes the procedure so that each Decision Tree is limited to a random sample of features of which to search. \n",
    "The number of features that can be searched at each split point (m) must be specified as a parameter to the algorithm.\n",
    "\n",
    "1. For classification a good default is: m = sqrt(p)\n",
    "2. For regression a good default is: m = p/3\n",
    "\n",
    "where p is the total no.of features available.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1597164316439"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}